== Vector-Stores und -Embeddings
include::locale/attributes-de.adoc[]
:imagesdir: images/


=== Theoretische Grundlagen
==== Was sind Vector Stores?
Ein Vector Store ist eine spezialisierte Datenbank, die darauf ausgelegt ist,
hochdimensionale Vektoren effizient zu speichern und abzufragen.
Vektoren sind mathematische Objekte, die aus einer Liste von numerischen
Werten bestehen und in der Regel in einem n-dimensionalen Raum dargestellt
werden. In der Welt der künstlichen Intelligenz und des maschinellen Lernens
werden Vektoren häufig verwendet, um komplexe Datenstrukturen wie Text,
Bilder und Videos zu repräsentieren.

Vector Stores bieten eine Reihe von Funktionen, die speziell für die
Handhabung von Vektoren entwickelt wurden. Dazu gehören:

- **Speichern und Verwalten grosser Mengen von Vektoren**: Vector Stores sind
    darauf ausgelegt, Millionen bis Milliarden von Vektoren effizient zu
    speichern, ohne dass die Leistung darunter leidet.
- **Schnelle und präzise Abfragen**: Durch spezialisierte Indizierungsmechanismen
    können Vector Stores schnelle und genaue Suchen nach ähnlichen Vektoren durchführen.
- **Integrationsmöglichkeiten**: Vector Stores sind oft mit anderen Tools
    und Bibliotheken kompatibel, die in der Datenanalyse und im maschinellen
    Lernen verwendet werden, sodass sie leicht in bestehende Workflows
    integriert werden können.

Ein praktisches Beispiel für die Anwendung von Vector Stores ist die Suche
nach semantisch ähnlichen Dokumenten. Hierbei werden Textdokumente in
Vektoren umgewandelt (z.B. durch die Verwendung von TF-IDF, Word2Vec oder BERT),
und diese Vektoren werden dann in einem Vector Store gespeichert.
Bei einer Abfrage kann der Vector Store effizient die Vektoren durchsuchen
und die Dokumente finden, die dem Abfragevektor am ähnlichsten sind.

==== Unterschiede zu traditionellen Datenbanken

Während traditionelle Datenbanken wie relationale Datenbanken (z.B. MySQL, PostgreSQL)
gut für strukturierte Daten und konventionelle Abfragen geeignet sind,
sind sie für den Umgang mit hochdimensionalen Vektordaten weniger optimal.
Hier sind einige wesentliche Unterschiede zwischen Vector Stores und
traditionellen Datenbanken:

- **Datenstruktur**: Traditionelle Datenbanken speichern Daten meist in Tabellenform,
    wobei jede Zeile einen Datensatz und jede Spalte ein Attribut darstellt.
    Im Gegensatz dazu speichern Vector Stores Daten als Vektoren in einem
    n-dimensionalen Raum.
- **Abfragemethoden**: Relationale Datenbanken verwenden SQL (Structured Query Language)
    für Abfragen, die auf Attributen und deren Werten basieren. Vector Stores
    hingegen verwenden spezialisierte Algorithmen wie _Approximate Nearest Neighbor_ (ANN)
    _Search_, um ähnliche Vektoren zu finden.
- **Leistung und Skalierbarkeit**: Bei der Suche nach ähnlichen Vektoren profitieren
    Vector Stores von speziellen Datenstrukturen (z.B. K-D Trees, Ball Trees)
    und Indexierungstechniken, die für hohe Dimensionsräume optimiert sind.
    Traditionelle Datenbanken sind in der Regel nicht für solche Anwendungsfälle
    optimiert und können bei grossen Datensätzen und komplexen Abfragen Leistungseinbussen
    erleben.
- **Anwendungsfälle**: Vector Stores werden häufig in Anwendungen verwendet,
    die semantische Ähnlichkeit oder inhaltliche Übereinstimmung benötigen, wie z.B. in
    Empfehlungsalgorithmen, Suchmaschinen und Bilderkennungssystemen.
    Traditionelle Datenbanken sind ideal für Anwendungen, die transaktionale
    Integrität, ACID-Eigenschaften (_Atomicity_, _Consistency_, _Isolation_, _Durability_)
    und strukturierte Datenmodelle benötigen.

Durch diese klaren Unterschiede bieten Vector Stores eine spezialisierte und effiziente
Lösung für moderne Anforderungen im Bereich der künstlichen Intelligenz und des
maschinellen Lernens.

=== Einführung in Vektorrepräsentationen

==== Bedeutung von Vektoren im maschinellen Lernen und in der künstlichen Intelligenz

Vektoren spielen eine zentrale Rolle im maschinellen Lernen und in der künstlichen
Intelligenz, da sie eine effiziente und mathematisch fundierte Methode bieten,
um komplexe Daten in einer Weise zu repräsentieren, die für Algorithmen leicht
verständlich und verarbeitbar ist. Ein Vektor ist eine geordnete Liste von
Zahlen, die einen Punkt in einem mehrdimensionalen Raum repräsentiert.
Diese Repräsentation ermöglicht es, Daten wie Texte, Bilder, Videos und
andere Formen von Informationen in ein Format zu überführen, das durch
mathematische und statistische Methoden analysiert und manipuliert werden kann.

Im maschinellen Lernen werden Vektoren verwendet, um Merkmale aus Daten
zu extrahieren und sie in einem mehrdimensionalen Raum zu positionieren.
Jedes Merkmal eines Datensatzes wird durch eine Dimension im Vektorraum
repräsentiert. Wenn alle Merkmale zusammengeführt werden, ergibt sich ein
Vektor, der den gesamten Datensatz beschreibt. Dieses Vorgehen ermöglicht
es Algorithmen, Muster in den Daten zu erkennen, Beziehungen zwischen
verschiedenen Datensätzen zu identifizieren und komplexe Vorhersagen zu treffen.

Ein herausragendes Beispiel für die Bedeutung von Vektoren ist die
Verarbeitung natürlicher Sprache (_Natural Language Processing_, NLP).
Hier werden Wörter oder Textsegmente in numerische Vektoren umgewandelt,
die semantische und syntaktische Informationen erfassen. Diese numerische
Darstellung ermöglicht es Maschinen, Text zu verstehen, Texte zu
klassifizieren, semantische Ähnlichkeiten zu berechnen und sogar
natürliche Sprache zu generieren.

Die Fähigkeit, Daten in Vektorform zu repräsentieren und zu analysieren,
hat viele bahnbrechende Entwicklungen in der künstlichen Intelligenz
ermöglicht, einschliesslich der Entwicklung von Sprachmodellen,
Bilderkennungssystemen, Empfehlungsalgorithmen und vielen anderen
Anwendungen. Im Wesentlichen bilden Vektoren die Grundlage für
die Verarbeitung und Analyse von Daten in modernen KI-Systemen.

==== Beispiele für Vektoren: Wortvektoren

Ein besonders anschauliches Beispiel für die Nutzung von Vektoren
im Bereich der KI sind Wortvektoren. Wortvektoren sind numerische
Repräsentationen von Wörtern, die semantische Beziehungen zwischen
Wörtern in einem kontinuierlichen Vektorraum erfassen.
Diese Repräsentationen ermöglichen es, die Bedeutung von Wörtern
auf eine Weise zu modellieren, die für maschinelle Lernalgorithmen
zugänglich und nützlich ist.

Zu den bekanntesten Methoden zur Erzeugung von Wortvektoren zählen
Word2Vec, GloVe (_Global Vectors for Word Representation_) und das
BERT-Modell (_Bidirectional Encoder Representations from Transformers_).

- **Word2Vec**: Word2Vec ist ein Algorithmus, der von Google entwickelt
    wurde und darauf abzielt, Wörter in einem kontinuierlichen Vektorraum zu
    positionieren, basierend auf ihrem Kontext in einem Textkorpus.
    Word2Vec verwendet zwei Architekturen: _Continuous Bag of Words_ (CBOW)
    und _Skip-gram_. CBOW sagt ein Wort basierend auf seinem Kontext voraus,
    während Skip-gram verwendet wird, um den Kontext basierend auf einem
    gegebenen Wort vorherzusagen. Das Ergebnis ist ein Satz von Vektoren,
    bei dem ähnliche Wörter in der Nähe voneinander im Vektorraum liegen.

- **GloVe**: GloVe ist ein weiteres populäres Verfahren zur Erzeugung von
    Wortvektoren, entwickelt von der Stanford University. Im Gegensatz zu
    Word2Vec, das lokale Kontexte verwendet, nutzt GloVe globale statistische
    Informationen aus der gesamten Textmenge. Es optimiert die Vektoren,
    indem es die Wahrscheinlichkeit modelliert, dass zwei Wörter zusammen
    in einem bestimmten Kontext auftreten. Dadurch werden Vektoren erzeugt,
    die eine hohe semantische Genauigkeit aufweisen und Wortbeziehungen
    besser erfassen.

- **BERT**: BERT ist ein tiefes, bidirektionales Transformationsmodell,
    das die Bedeutung von Wörtern und deren Kontext in einem Text erfasst.
    Es verwendet eine maskierte Wortprädiktionsaufgabe sowie eine
    Satzpaar-Klassifizierungsaufgabe, um kontextuelle Repräsentationen
    von Wörtern zu lernen. BERT-Vektoren sind kontextspezifisch,
    was bedeutet, dass dasselbe Wort unterschiedliche Vektoren in
    verschiedenen Kontexten haben kann. Diese Eigenschaft macht BERT
    besonders leistungsfähig bei Aufgaben der natürlichen Sprachverarbeitung.

Dank dieser Methoden können Wortvektoren verwendet werden, um semantische
Ähnlichkeiten zu berechnen, Wörter zu klassifizieren, Textdokumente
zu clustern und viele andere NLP-bezogene Aufgaben zu lösen.
Wortvektoren sind ein hervorragendes Beispiel dafür, wie
Vektorrepräsentationen die Fähigkeit von Maschinen, natürliche
Sprache und ihre Bedeutungen zu verstehen, revolutioniert haben.

=== Vector Embeddings

==== Was sind Embeddings?
Embeddings sind eine Möglichkeit, komplexe Daten wie Wörter, Sätze, Bilder
oder andere Elemente numerisch zu repräsentieren, indem sie in einen
niedrigdimensionalen Vektorraum eingebettet werden. Diese Vektoren,
auch Embeddings genannt, erfassen die relevanten Merkmale und
Beziehungen der ursprünglichen Daten in einer hoch verdichteten
und manchmal weniger dimensionalen Form. Embeddings spielen eine
zentrale Rolle in vielen Anwendungen des maschinellen Lernens und
der künstlichen Intelligenz, da sie die Grundlage für die Verarbeitung
und Analyse komplexer Daten bieten.

Das grundlegende Ziel von Embeddings ist es, semantisch ähnliche
Datenpunkte näher zusammen zu positionieren und unähnliche Datenpunkte
weiter auseinander zu halten. Beispielsweise könnten in einem
Wortembedding-Raum die Vektoren für "König" und "Königin" nahe
beieinander liegen, weil sie semantisch ähnlich sind, während
"König" und "Auto" weiter getrennt sind, da sie unterschiedliche
Bedeutungen haben.

Ein wesentlicher Vorteil von Embeddings ist ihre Fähigkeit,
hohe Dimensionen und komplexe Datenstrukturen in handhabbare
und recheneffiziente Vektoren zu überführen. Diese Transformation
erleichtert es maschinellen Lernmodellen, die Daten zu
verarbeiten und Muster oder Beziehungen zu erkennen, die in
den Rohdaten nicht offensichtlich sind.

Embeddings werden häufig in der Verarbeitung natürlicher
Sprache (_Natural Language Processing_, NLP) verwendet,
um Wörter und Sätze in dichte Vektoren umzuwandeln.
Diese Vektoren erfassen die semantischen und syntaktischen
Merkmale der Sprache, sodass maschinelle Lernmodelle Text
verstehen und analysieren können. Neben NLP-Anwendungen
finden Embeddings auch in der Bilderkennung, der
Empfehlungssysteme und vielen anderen Bereichen Anwendung,
in denen hochdimensionale Daten verarbeitet werden müssen.

==== Methoden zur Erzeugung von Embeddings

Es gibt verschiedene Methoden zur Erzeugung von Embeddings,
die je nach Anwendungsfall und Datenstruktur eingesetzt werden.
Hier sind einige der bekanntesten und am häufigsten verwendeten Ansätze:

- **Word2Vec**: Word2Vec ist eine Technik, die von Google entwickelt
    wurde, um Wörter aus grossen Textkorpora in kontinuierliche
    Vektoren zu überführen. Es gibt zwei Hauptarchitekturen in
    Word2Vec: _Continuous Bag of Words_ (CBOW) und _Skip-gram_.
    CBOW sagt ein Wort basierend auf seinem Kontext voraus,
    während Skip-gram den Kontext basierend auf einem zentralen
    Wort vorhersagt. Diese architektonischen Ansätze ermöglichen
    es dem Modell, semantische Beziehungen zwischen Wörtern zu
    erlernen und Wörter mit ähnlicher Bedeutung in der Nähe
    im Vektorraum zu platzieren.

- **GloVe (_Global Vectors for Word Representation_)**: GloVe ist
    ein weiteres Verfahren zur Erzeugung von Wortvektoren, das von
    der Stanford University entwickelt wurde. GloVe kombiniert
    globale statistische Informationen aus dem gesamten Textkorpus
    mit der lokalen Kontextinformation, um hochpräzise Wortvektoren
    zu erzeugen. Dies wird erreicht, indem die Wahrscheinlichkeit
    modelliert wird, dass Wörter in bestimmten Kontexten zusammen
    auftreten. Das Resultat sind Vektoren, die eine starke
    semantische Bedeutung besitzen und präzise Wortbeziehungen erfassen.

- **FastText**: FastText, entwickelt von Facebook, erweitert das
    Word2Vec-Modell, indem es Wörter als N-Gramme (Teilstücke von Wörtern)
    darstellt. Durch die Verwendung von Subwortinformationen kann
    FastText effektiver mit seltenen und unbekannten Wörtern umgehen,
    da auch unbekannte Wörter in ihre N-Gramme zerlegt und deren Vektoren
    summiert werden. Dies führt zu robusteren Wortvektoren,
    insbesondere für Sprachen mit komplexen Morphologien.

- **BERT (_Bidirectional Encoder Representations from Transformers_)**: BERT
    ist ein hochentwickeltes Sprachmodell, das von Google entwickelt
    wurde und auf der Transformer-Architektur basiert. BERT erzeugt
    kontextabhängige Wortvektoren, indem es den bidirektionalen
    Kontext eines Wortes in einem Satz berücksichtigt.
    Dies ermöglicht es dem Modell, tiefere semantische Bedeutungen
    zu erfassen und in Aufgaben der natürlichen Sprachverarbeitung
    (z.B. Fragebeantwortung, Named Entity Recognition) aussergewöhnlich
    gut abzuschneiden. BERT verwendet _Masked Language Modeling_ (MLM)
    und _Next Sentence Prediction_ (NSP) zur Erzeugung hochwertiger Embeddings.

- **Sentence-BERT (SBERT)**: Sentence-BERT erweitert das BERT-Modell,
    indem es ganze Sätze in dichte Vektoren einbettet, die semantische
    Informationen über den gesamten Satz hinweg erfassen.
    Dies ermöglicht es, semantische Ähnlichkeiten auf Satz-
    oder Textabsatzebene zu berechnen und ist besonders nützlich
    für Textklassifikation, Informationsabruf und Satzähnlichkeitsaufgaben.

- **DeepWalk**: DeepWalk ist eine Methode zur Erzeugung von Embeddings
    für Netzwerke oder Graphen. Sie transformiert Grapheninformationen
    in niedrigdimensionale Vektoren, wobei zufällige Wanderungen
    (_Random Walks_) auf dem Graphen durchgeführt und diese als Sätze
    behandelt werden. Diese Sätze werden anschließend mit Techniken
    wie Word2Vec verarbeitet, um Knotenembeddings zu erzeugen, die
    die Struktur und Beziehungen im Graphen widerspiegeln.

Diese Methoden zur Erzeugung von Embeddings haben bedeutende Fortschritte
in verschiedenen Anwendungen ermöglicht, indem sie die Rohdaten in
eine Form überführen, die recheneffizient ist und tiefere semantische
Bedeutungen erfassen kann. Die Auswahl der geeigneten Methode hängt
von der speziellen Anwendung und den Datenanforderungen ab.

=== Verwendung von Vector Embeddings
==== Anwendungsbereiche von Embeddings

Vector Embeddings finden Einsatz in einer Vielzahl von Anwendungen
im Bereich der künstlichen Intelligenz und des maschinellen Lernens.
Ihre Fähigkeit, komplexe und hochdimensionale Daten in dichte,
numerische Vektoren zu verwandeln, ermöglicht es, verschiedene
Arten von Informationen zu vergleichen, zu klassifizieren und
zu analysieren. Hier sind einige der wichtigsten Anwendungsbereiche
von Embeddings:

- **Informationsabruf und -wiedergewinnung**: Embeddings werden
    häufig verwendet, um Dokumente oder Datenpunkte zu repräsentieren,
    die in grossen Datenmengen gespeichert sind. Durch die
    Berechnung von Ähnlichkeiten zwischen Embeddings können
    relevante Dokumente effizienter abgerufen werden, was nützlich
    ist in Suchmaschinen, Fragebeantwortungssystemen und bei
    der Indexierung von Daten.

- **Textklassifikation**: Eine der weit verbreiteten Anwendungen
    von Embeddings ist die Textklassifikation. Hierbei werden
    Textdaten in Vektoren umgewandelt, die dann als Input für
    Klassifikationsmodelle dienen. Dies ermöglicht es, Texte
    automatisch in Kategorien einzuordnen, wie z.B. Spam-Erkennung
    in E-Mails oder Sentiment-Analyse in sozialen Medien.

- **Ähnlichkeitssuche und Clustering**: Embeddings werden auch
    in der Ähnlichkeitssuche und beim Clustering verwendet.
    Bei der Ähnlichkeitssuche werden Vektoren verwendet,
    um die Ähnlichkeit zwischen Datenpunkten zu berechnen
    und ähnliche Elemente zu finden. Beim Clustering werden
    Datenpunkte basierend auf ihren Vektorrepräsentationen
    in Gruppen unterteilt, um Muster und Strukturen in den
    Daten zu erkennen.

- **Empfehlungssysteme**: Moderne Empfehlungssysteme nutzen
    Embeddings, um Benutzerpräferenzen und Artikelmerkmale
    darzustellen. Durch die Berechnung der Ähnlichkeiten
    zwischen Benutzer- und Artikel-Embeddings können personalisierte
    Empfehlungen generiert werden, die auf den Präferenzen
    des Benutzers basieren.

- **Bilderkennung und -klassifikation**: In der Computer Vision
    werden Bilder in Embeddings umgewandelt, die visuelle Merkmale
    und Inhalte erfassen. Diese Embeddings können verwendet werden,
    um Bilder zu klassifizieren, ähnliches Bildmaterial zu finden
    oder Objekte in Bildern zu erkennen.

- **Sprachübersetzung und Sprachgenerierung**: Embeddings spielen
    eine zentrale Rolle in der maschinellen Übersetzung und
    Sprachgenerierung. Modelle wie BERT und GPT verwenden
    Embeddings, um den Kontext und die Bedeutung von Wörtern
    und Sätzen zu erfassen, was die Übersetzung und Erzeugung
    natürlicher Sprache verbessert.

- **Graphenanalyse**: Embeddings werden auch verwendet, um Knoten
    in Graphen zu repräsentieren. Dies ermöglicht die Analyse
    von Netzwerkstrukturen, z.B. zur Erkennung von
    Gemeinschaften, zur Vorhersage von Links oder zur
    Klassifizierung von Knoten basierend auf ihrer Einbettung.


==== Praktische Beispiele mit Pseudocode
Um die praktische Anwendung von Embeddings zu veranschaulichen,
betrachten wir einige Beispiele, die zeigen, wie Embeddings
in verschiedenen Szenarien genutzt werden können.

- **Ähnlichkeitssuche im Text**:
  Ein häufiges Anwendungsgebiet von Embeddings ist die Berechnung der semantischen Ähnlichkeit zwischen Texten. Hier ist ein Pseudocode-Beispiel, das zeigt, wie Embeddings verwendet werden können, um ähnliche Sätze zu finden:
+
[source,python]
----
# Pseudocode zur Berechnung der semantischen Ähnlichkeit zwischen Texten
import embedding_library

# Texte definieren
text1 = "Das Wetter ist heute sonnig."
text2 = "Es ist ein heller und sonniger Tag."
text3 = "Ich liebe kaltes Winterwetter."

# Erzeugen von Embeddings
embedding1 = embedding_library.create_embedding(text1)
embedding2 = embedding_library.create_embedding(text2)
embedding3 = embedding_library.create_embedding(text3)

# Berechnung der Ähnlichkeiten (z.B. Kosinus-Ähnlichkeit)
similarity_1_2 = embedding_library.cosine_similarity(embedding1, embedding2)
similarity_1_3 = embedding_library.cosine_similarity(embedding1, embedding3)

# Ausgabe der Ergebnisse
print(f"Ähnlichkeit zwischen Text 1 und Text 2: {similarity_1_2}")
print(f"Ähnlichkeit zwischen Text 1 und Text 3: {similarity_1_3}")
----

- **Textklassifikation mit Embeddings**: In diesem Beispiel verwenden wir Embeddings,
um Texte in Kategorien einzuordnen. Der Pseudocode zeigt, wie ein Klassifikationsmodell
mit Embeddings trainiert wird:
+
[source,python]
----
# Pseudocode zur Textklassifikation
import embedding_library
from machine_learning_library import Classifier

# Trainingsdaten
texts = ["Diese E-Mail ist Spam.", "Dies ist eine legitime E-Mail.", ...]
labels = ["Spam", "Ham", ...]

# Erzeugen von Embeddings für die Trainingsdaten
embeddings = [embedding_library.create_embedding(text) for text in texts]

# Trainingsdatenset erstellen
train_data = (embeddings, labels)

# Klassifikationsmodell erstellen und trainieren
classifier = Classifier()
classifier.train(train_data)

# Klassifikation eines neuen Textes
new_text = "Dies ist eine neue E-Mail, die klassifiziert werden soll."
new_embedding = embedding_library.create_embedding(new_text)
prediction = classifier.predict(new_embedding)

# Ausgabe der Klassifikation
print(f"Die Klassifikation des neuen Textes ist: {prediction}")
----

- **Empfehlungssystem mit Embeddings**: Hier ein Pseudocode-Beispiel zur
Implementierung eines einfachen Empfehlungssystems, das Artikel basierend
auf Benutzerpräferenzen empfiehlt:
+
[source,python]
----
# Pseudocode für ein Empfehlungssystem
import embedding_library
from recommendation_library import Recommender

# Beispiel-Benutzerdaten
user_preferences = ["Artikel 1", "Artikel 2", ...]

# Erzeugen von Embeddings für die Benutzerdaten
user_embeddings = [embedding_library.create_embedding(item) for item in user_preferences]

# Artikel-Datenbank
articles = ["Artikel A", "Artikel B", "Artikel C", ...]
article_embeddings = [embedding_library.create_embedding(article) for article in articles]

# Empfehlungssystem erstellen und trainieren
recommender = Recommender()
recommender.train(user_embeddings, article_embeddings)

# Empfehlungen generieren
recommendations = recommender.recommend(user_embeddings)

# Ausgabe der Empfehlungen
for recommendation in recommendations:
    print(recommendation)
----

Diese Beispiele veranschaulichen die praktischen Anwendungen von Embeddings
in verschiedenen Szenarien. Durch das Umwandeln von Daten in dichte
numerische Vektoren ermöglichen Embeddings eine effiziente Verarbeitung,
Analyse und Vorhersage in einer Vielzahl von maschinellen Lernanwendungen.


=== Einführung in Vector Stores

==== Architektur und Funktionsweise von Vector Stores

Vector Stores sind spezialisierte Datenbanksysteme, die entwickelt
wurden, um hochdimensionale Vektoren effizient zu speichern
und abzufragen. Die zugrundeliegende Architektur und
Funktionsweise von Vector Stores sind darauf abgestimmt,
die Herausforderungen bei der Verarbeitung und Suche in
grossen Vektormengen zu bewältigen. Hier ein detaillierter
Überblick über die Architektur und Funktionsweise:

- **Datenstrukturen und Speicher**:
  Vector Stores nutzen spezialisierte Datenstrukturen und
    Speichertechniken, um Vektoren effizient zu speichern
    und abzurufen. Dazu zählen _In-Memory_-Speicher,
    Festplattenspeicher oder hybride Ansätze, die eine
    Mischung aus beiden verwenden. Die Wahl der Datenstruktur
    hängt von den Anforderungen an die Lade- und Abfragezeiten
    sowie der Menge und Grösse der zu speichernden Vektoren ab.

- **Indexierung**: Eine der Schlüsselkomponenten eines Vector Stores ist das
    Indexierungssystem. Da Vektoren im Allgemeinen hochdimensional
    sind, ist eine einfache lineare Suche ineffizient.
    Stattdessen verwenden Vector Stores fortschrittliche
    Indexierungsalgorithmen wie _Approximate Nearest Neighbor_ (ANN)
    oder strukturbasierte Methoden wie _k-d_ Trees, _Ball Trees_ oder
    HNSW (_Hierarchical Navigable Small World Graphs_).
    Diese Techniken reduzieren die Abfragezeiten erheblich, indem
    sie einen Teil der Vektorraumsuche vorab berechnen und nur
    relevante Teilräume durchsuchen.

- **Abfragesystem**: Vector Stores sind darauf ausgelegt, hochdimensionale Abfragen schnell
    und präzise zu beantworten. Die typischen Abfragen umfassen die
    _K-Nearest Neighbors_ (K-NN) Suche, die Suche nach dem nächsten
    Nachbarn oder die Berechnung der kosinusbasierten Ähnlichkeit.
    Diese Abfragen werden durch spezialisierte Algorithmen und
    Datenstrukturen ermöglicht, die die komplexen Berechnungen
    effizient durchführen.

- **Skalierbarkeit und Verteilte Systeme**: Vector Stores sind oft skalierbar
    und können grosse Datenmengen verarbeiten, indem sie auf verteilte
    Systeme zurückgreifen. Dies ermöglicht es ihnen, Vektoren über mehrere
    Knoten zu verteilen und parallele Abfragen durchzuführen, um die
    Leistung und Skalierbarkeit zu erhöhen. Systeme wie FAISS von Facebook
    oder Milvus sind Beispiele für skalierbare, verteilte Vector Stores.

- **Integrations- und Schnittstellen**: Vector Stores bieten oft APIs und
    Integrationsmöglichkeiten für verschiedene Programmiersprachen und
    Frameworks. Dies erleichtert die Integration in bestehende Workflows
    und Anwendungen. Außerdem unterstützen sie häufig Schnittstellen zu
    Machine Learning Bibliotheken wie TensorFlow oder PyTorch, um eine
    nahtlose Integration mit maschinellen Lernmodellen zu ermöglichen.

Durch diese architektonischen Merkmale bieten Vector Stores eine spezialisierte
und effiziente Lösung für die Speicherung und Abfrage hochdimensionaler
Vektoren, die in vielen modernen Anwendungen benötigt werden.

==== Einsatzgebiete von Vector Stores

Vector Stores finden in einer Vielzahl von Anwendungsgebieten Einsatz,
die auf die Analyse und Verarbeitung hochdimensionaler Daten angewiesen
sind. Hier sind einige der wichtigsten Einsatzgebiete:

- **Ähnlichkeitssuche und -wiedererkennung**: Ein weit verbreitetes
    Einsatzgebiet von Vector Stores ist die Ähnlichkeitssuche. Dabei
    werden Vektoren verwendet, um Datenelemente zu vergleichen und
    ähnliche Einträge zu finden. Dies ist besonders nützlich in
    Suchmaschinen, bei der Bild- und Videoerkennung sowie in
    Musiksuchdiensten. Zum Beispiel kann ein Vector Store verwendet
    werden, um Bilder in einer großen Datenbank zu durchsuchen
    und diejenigen zu finden, die einem gegebenen Bild am ähnlichsten sind.

- **Empfehlungssysteme**: Vector Stores spielen eine zentrale Rolle in
    modernen Empfehlungssystemen. Sie ermöglichen die Speicherung
    und Abfrage von Vektoren, die Benutzerpräferenzen und Artikelmerkmale
    repräsentieren. Durch die Berechnung der Ähnlichkeiten zwischen
    Benutzer- und Artikelvektoren können personalisierte Empfehlungen
    generiert werden, was in E-Commerce-Plattformen, Streaming-Diensten
    und Social Media von grosser Bedeutung ist.

- **Dokumenten- und Textsuche**: In der Text- und Dokumentensuche
    werden Vector Stores verwendet, um Textdokumente in Form von
    Vektoren zu speichern und schnell auf relevante Informationen
    zuzugreifen. Dies ist besonders nützlich in Fragebeantwortungssystemen,
    bei der automatischen Klassifikation von Texten und in der semantischen
    Suche, wo nicht nur die wortgenaue Übereinstimmung, sondern auch die
    semantische Bedeutung des Textes beachtet wird.

- **Bilderkennung und -analyse**: In der Computer Vision werden Vector
    Stores eingesetzt, um Bildmerkmale zu speichern und abzufragen.
    Diese Vektoren können aus Rohbildern extrahiert werden und
    repräsentieren visuelle Merkmale wie Kanten, Farben und
    Texturen. Vector Stores können dann verwendet werden, um
    ähnliche Bilder zu finden oder Bildklassifikation durchzuführen.

- **Betrugserkennung und Anomalieerkennung**: Vector Stores können auch
    für die Betrugserkennung und Anomalieerkennung in Finanz- und
    Sicherheitssystemen verwendet werden. Vektoren werden verwendet,
    um Transaktionsdaten oder Benutzermuster zu repräsentieren, und
    können schnell untersucht werden, um abnormale Aktivitäten oder
    Betrugsversuche zu identifizieren.

- **Genomik und Bioinformatik**: In der Bioinformatik und Genomik
    ermöglichen Vector Stores die Speicherung und Analyse
    hochdimensionaler Daten wie DNA-Sequenzen oder Proteinstrukturen.
    Dies erleichtert die Suche nach genetischen Ähnlichkeiten und
    Mustern, die für die Forschung und Entwicklung neuer Medikamente
    von entscheidender Bedeutung sind.

- **Sprach- und Audioverarbeitung**: Vector Stores werden auch in der
    Sprach- und Audioverarbeitung verwendet, um Audio- und Sprachdaten
    zu analysieren. Vektoren können verwendet werden, um Merkmale
    von Audiodateien zu extrahieren, wie z.B. Tonhöhe, Lautstärke
    und Spektrogramme, und diese Daten können für Aufgaben wie
    Spracherkennung, Speaker Identification und Audio-Clustering
    verwendet werden.

Diese vielfältigen Einsatzgebiete zeigen, wie vielseitig und
leistungsfähig Vector Stores sind, wenn es darum geht, hochdimensionale
Daten zu analysieren und zu verarbeiten. Sie bieten spezifische
Lösungen für verschiedene Branchen und Anwendungsfälle, die auf die
effiziente Speicherung und Abfrage von Vektoren angewiesen sind.


=== Unterschied zwischen Vector Embeddings und Vector Stores

Vector Embeddings und Vector Stores sind zwei unterschiedliche,
aber miteinander verbundene Konzepte, die in der Welt der künstlichen
Intelligenz und des maschinellen Lernens eine wichtige Rolle spielen.
Hier ist eine detaillierte Erklärung der beiden Begriffe und
ihrer Unterschiede:

==== Vector Embeddings

**Vector Embeddings** sind eine Methode zur Repräsentation komplexer
Daten wie Wörter, Sätze, Bilder oder andere Elemente als hochdimensionale
Vektoren in einem kontinuierlichen Vektorraum. Diese Vektoren fassen
die wichtigsten Merkmale und Beziehungen der Originaldaten in einer
kompakten und mathematisch verwertbaren Form zusammen.
Die Hauptziele von Embeddings sind es, die semantischen Beziehungen
zwischen Datenpunkten zu erfassen und gleichzeitig die Dimension der
Daten zu reduzieren, um sie recheneffizienter zu machen.

*Merkmale von Vector Embeddings:*
- *Hochdimensionale Vektoren*: Embeddings repräsentieren Daten in einem n-dimensionalen Raum, wobei jede Dimension eine spezifische Eigenschaft oder ein Merkmal der Daten beschreibt.
- *Semantische Bedeutung*: Eingebettete Vektoren erfassen die semantischen Beziehungen zwischen Datenpunkten, sodass ähnlich bedeutende Daten nahe beieinander im Vektorraum liegen.
- *Reduzierung der Komplexität*: Embeddings transformieren komplexe Daten in eine weniger dimensionale Form, was die Verarbeitung durch Maschinen effizienter macht.

*Beispiele für Vector Embeddings:*
- *Word2Vec und GloVe*: Diese Techniken erzeugen Vektoren, die semantische Beziehungen zwischen Wörtern erfassen.
- *BERT*: Ein Modell, das kontextspezifische Wort- und Satz-Vektoren erzeugt und in der natürlichen Sprachverarbeitung genutzt wird.

==== Vector Stores

**Vector Stores** sind spezialisierte Datenbanken, die entwickelt wurden,
um hochdimensionale Vektoren effizient zu speichern, zu verwalten und
abzufragen. Während Vector Embeddings die numerische Darstellung der
Daten liefern, bieten Vector Stores die Infrastruktur, um diese Vektoren
zu speichern und schnelle Abfragen zu ermöglichen, insbesondere in
Szenarien mit großen Datenmengen und der Notwendigkeit, ähnliche oder
relevante Vektoren zu finden.

*Merkmale von Vector Stores:*
- *Speicherung von Vektoren*: Vector Stores sind darauf ausgelegt, große Mengen an hochdimensionalen Vektoren effizient zu speichern.
- *Effiziente Abfrage- und Suchmethoden*: Sie nutzen spezialisierte Indexierungs- und Suchalgorithmen, um schnelle und präzise Suchen nach ähnlichen Vektoren durchzuführen.
- *Skalierbarkeit*: Vector Stores können oft auf verteilten Systemen betrieben werden, um große Datenmengen zu handhaben und die Leistung zu maximieren.

*Beispiele für Vector Stores:*
- *FAISS*: Ein von Facebook entwickelter Vector Store, der schnelle Ähnlichkeitssuchen in großen Vektormengen ermöglicht.
- *Milvus*: Ein Open-Source-Vector Store, der für hochdimensionale Daten und maschinelles Lernen entwickelt wurde.

==== Unterschiede zwischen Vector Embeddings und Vector Stores

1. **Ziel und Funktion**:
   - *Vector Embeddings*: Transformieren komplexe Daten in hochdimensionale Vektoren, die semantische Eigenschaften und Beziehungen erfassen.
   - *Vector Stores*: Bieten die Infrastruktur zur Speicherung und Abfrage dieser Vektoren, insbesondere für große Datensätze und schnelle Suchanfragen.

2. **Aufgaben und Anwendungsbereiche**:
   - *Vector Embeddings*: Werden hauptsächlich durch maschinelle Lernmodelle erzeugt und in verschiedenen KI-Anwendungen wie NLP, Computer Vision und Empfehlungssystemen verwendet.
   - *Vector Stores*: Dienen als spezialisierte Datenbanken, die diese Vektoren effizient speichern und schnelle Abfragen ermöglichen, was in Suchmaschinen, Empfehlungssystemen und anderen Anwendungen genutzt wird.

3. **Technologie und Implementierung**:
   - *Vector Embeddings*: Nutzen Modelle wie Word2Vec, GloVe, BERT etc., um die Vektoren zu erzeugen.
   - *Vector Stores*: Verwenden spezialisierte Algorithmen und Datenstrukturen wie k-d Trees, HNSW und ANN zur effizienten Speicherung und Suche nach Vektoren.

==== Zusammenfassung

Zusammenfassend lässt sich sagen, dass Vector Embeddings und Vector Stores
komplementäre Technologien sind. Während Embeddings dafür sorgen,
dass komplexe Daten in numerische Vektoren umgewandelt werden,
speichern und verwalten Vector Stores diese Vektoren und ermöglichen
ihre effiziente Abfrage. Beide Technologien spielen eine entscheidende
Rolle in modernen maschinellen Lern- und KI-Systemen und tragen dazu bei,
die Verarbeitung und Analyse großer und komplexer Datensätze zu optimieren.


=== Praktische Implementierung

==== Auswahl von Tools und Bibliotheken

Bei der Auswahl von Tools und Bibliotheken für die Erstellung und Nutzung von Vector Stores gibt es mehrere Optionen, die sich durch unterschiedliche Eigenschaften und Anwendungsfälle auszeichnen. Hier sind einige der bekanntesten und am häufigsten verwendeten Tools und Bibliotheken:

- **FAISS (Facebook AI Similarity Search)**:
  FAISS ist eine Bibliothek von Facebook AI Research, die entwickelt wurde, um schnelle Ähnlichkeitssuchen in großen Vektormengen zu ermöglichen. FAISS unterstützt sowohl exakte als auch Näherungssuchen und ist für hohe Leistung und Skalierbarkeit optimiert. Es bietet verschiedene Algorithmen und Indexierungsmechanismen, darunter k-d Trees, vorgefertigte Vektorencluster und HNSW (Hierarchical Navigable Small World Graphs).

- **Annoy (Approximate Nearest Neighbors Oh Yeah)**:
  Annoy ist eine von Spotify entwickelte Bibliothek, die Approximate Nearest Neighbor Suchen mit einer Mischung aus Random-Projection-Bäumen und k-d Bäumen ermöglicht. Annoy ist auf sehr große Datensätze und minimalen Speicherbedarf optimiert und bietet eine besonders effiziente Implementierung für das Einfügen und Abfragen von Vektoren.

- **Milvus**:
  Milvus ist ein hoch skalierbarer Open-Source-Vector Store, der entwickelt wurde, um Milliarden von Vektoren zu speichern und abzufragen. Milvus nutzt fortgeschrittene Indexierungstechniken wie IVF (Inverted File System), HNSW und PQ (Product Quantization), um schnelle und genaue Suchvorgänge zu ermöglichen. Milvus unterstützt verteilte Systeme und bietet eine benutzerfreundliche API.

- **Elasticsearch mit Vector Search**:
  Elasticsearch ist eine weit verbreitete Such- und Analyse-Engine, die auch die Suche nach Vektor-ähnlichen Dokumenten unterstützt. Mit der Einführung von Densen Vectors Indexing und K-NN (k-Nearest Neighbor) Suchfunktionen kann Elasticsearch als einfacher Vector Store fungieren und ermöglicht eine nahtlose Integration in bestehende Elasticsearch-Workflows.

- **PostgreSQL mit pgvector**:
  PostgreSQL, eine leistungsstarke relationale Datenbank, hat mit der Erweiterung pgvector Unterstützung für Vektordaten und -abfragen gewonnen. Diese Erweiterung ermöglicht es, Vektor-ähnliche Datenbanken in PostgreSQL zu erstellen und unterstützt K-NN-Suchen, was PostgreSQL zu einer vielseitigen Lösung für kleinere Vektordatenanwendungen macht.

==== Erstellung eines Vector Stores

Die Erstellung eines Vector Stores beinhaltet mehrere Schritte, von der Vorbereitung der Daten bis zur Implementierung der Suchlogik. Hier ist ein exemplarischer Ablauf zur Erstellung eines einfachen Vector Stores mit FAISS:

1. **Installationsvorbereitung**:
   Zunächst müssen die erforderlichen Bibliotheken installiert werden. Für FAISS kann dies durch die Ausführung eines einfachen Pip-Befehls erfolgen:
+
[source,shell]
----
pip install faiss-cpu
----

2. **Datenvorbereitung**: Bereiten Sie Ihre Daten vor, indem Sie sie in numerische Vektoren umwandeln. Dies kann mit einer Embedding-Methode wie Word2Vec, GloVe oder BERT erfolgen. Hier ein Beispiel für die Erzeugung von Embeddings mit dem Word2Vec-Modell:
+
[source,python]
----
from gensim.models import Word2Vec

# Beispieltextdaten
sentences = [["das", "wetter", "ist", "heute", "sonnig"], ["es", "ist", "ein", "heller", "tag"]]

# Training des Word2Vec-Modells
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Erzeugen von Vektoren (Embeddings) für die Wörter
vector_sonnig = model.wv['sonnig']
vector_tag = model.wv['tag']
----

3. **Erstellung des Vector Stores mit FAISS**: Sobald die Vektoren vorbereitet sind, können sie in einem FAISS-Index gespeichert werden. Hier ist ein Beispiel zur Erzeugung eines FAISS-Indexes und Einfügen der Vektoren:
+
[source,python]
----
import faiss
import numpy as np

# Erstellen eines FAISS-Index
d = 100  # Dimension der Vektoren
index = faiss.IndexFlatL2(d)  # L2-Distanzmetrikenstandard

# Konvertieren der Vektoren in ein Numpy-Array
vectors = np.array([vector_sonnig, vector_tag])

# Hinzufügen der Vektoren zum Index
index.add(vectors)

# Überprüfung der Anzahl der im Index gespeicherten Vektoren
print(f"Anzahl der Vektoren im Index: {index.ntotal}")
----

4. **Durchführung von Abfragen*: Mit dem erstellten Vector Store können nun Abfragen durchgeführt werden, um ähnliche Vektoren zu finden. Hier ein Beispiel für eine K-NN-Suche:
+
[source,python]
----
# Abfragevektor
query_vector = model.wv['heute']

# Durchführung einer K-NN-Suche (Suche nach den 2 nächsten Nachbarn)
k = 2
D, I = index.search(np.array([query_vector]), k)

# Ausgabe der Ergebnisse
print(f"Indizes der nächsten Nachbarn: {I}")
print(f"Distanzen zu den nächsten Nachbarn: {D}")
----

Durch diese Schritte können Sie einen einfachen Vector Store erstellen
und für die effiziente Abfrage hochdimensionaler Vektoren verwenden.
Die Wahl des Tools und der Bibliotheken hängt von den spezifischen
Anforderungen und dem Anwendungsfall ab.

=== Hands-on Übung
==== Übung 1: Erzeugung von Embeddings mit Word2Vec
==== Übung 2: Implementierung eines einfachen Vector Stores mit FAISS

=== Integration mit LangChain

==== Verknüpfung von Vector Stores und LLMs

Die Verknüpfung von Vector Stores mit großen Sprachmodellen (_Large Language Models_, LLMs) wie GPT-3 eröffnet leistungsfähige Anwendungsmöglichkeiten, insbesondere im Bereich der natürlichen Sprachverarbeitung und des kontextuellen Verständnisses. LangChain ist eine Bibliothek, die diese Verknüpfung erleichtert, indem es eine robuste Schnittstelle zur nahtlosen Integration von Vector Stores und LLMs bietet. Die Hauptvorteile dieser Integration umfassen:

- **Kontextuelle Suche**: Durch die Nutzung von Vector Stores können Suchabfragen kontextuell erweitert werden. Die LLMs extrahieren den relevanten Kontext aus Textdaten und verwenden Embeddings, um die Ergebnisse in einem Vector Store effizient zu durchsuchen.
- **Textgenerierung und Vervollständigung**: LLMs können nicht nur Texte analysieren, sondern auch generieren. Mit Vector Stores können diese generierten Texte in hochdimensionale Vektoren umgewandelt und gespeichert werden, um sie später für Abfragen oder Kontextvervollständigungen zu nutzen.
- **Verbesserte Empfehlungsalgorithmen**: Durch die Kombination von Embeddings und LLMs können Empfehlungsalgorithmen verbessert werden. Die Vektoren aus den LLMs repräsentieren die Präferenzen und Interessen der Benutzer genauer und ermöglichen präzisere Empfehlungen.

LangChain stellt die Werkzeuge bereit, um diese Funktionen nahtlos zu integrieren und zu nutzen. Die Bibliothek bietet Unterstützung für verschiedene Vector Store-Technologien und ermöglicht es, LLMs effizient zu trainieren, zu fine-tunen und in bestehenden Workflows zu verwenden.

==== Erstellung einer Beispielanwendung mit LangChain und Vector Stores

Um die praktische Anwendung von LangChain mit Vector Stores zu veranschaulichen, betrachten wir eine Beispielanwendung, die die kontextuelle Suche in einer Dokumentendatenbank ermöglicht. Wir werden GPT-3 als LLM und FAISS als Vector Store verwenden. Hier sind die Schritte zur Erstellung dieser Anwendung:

1. **Installationsvorbereitung**:
Zunächst müssen die erforderlichen Bibliotheken installiert werden:
+
[source,shell]
----
pip install openai langchain faiss-cpu
----

2. **Initialisierung von GPT-3 und FAISS**:
Wir initialisieren GPT-3 für die Erzeugung von Text-Embeddings und FAISS als Vector Store:
+
[source,python]
----
import openai
import faiss
import numpy as np
from langchain import LangChain

# GPT-3 API-Schlüssel setzen
openai.api_key = 'YOUR_API_KEY'

# Language Chain initialisieren
langchain = LangChain()

# FAISS-Index initialisieren
d = 768  # Dimension der GPT-3 Vektoren
index = faiss.IndexFlatL2(d)

# Beispiel-Dokumente
documents = [
    "Das Wetter ist sonnig und angenehm.",
    "Es regnet heute den ganzen Tag.",
    "Morgen wird es bewölkt sein."
]

# Embeddings erstellen und in den FAISS-Index einfügen
embeddings = []

for doc in documents:
    response = openai.Embedding.create(input=doc, model="text-embedding-ada-002")
    embedding = response['data'][0]['embedding']
    embeddings.append(embedding)

index.add(np.array(embeddings))

# Anzahl der Vektoren im Index überprüfen
print(f"Anzahl der Vektoren im Index: {index.ntotal}")
----

3. **Durchführung einer kontextuellen Suche**:
Um eine kontextuelle Suche durchzuführen, verwenden wir GPT-3 zur Erzeugung eines Query-Embeddings und FAISS zum Finden der ähnlichsten Dokumente:
+
[source,python]
----
# Abfrage definieren und Embedding erstellen
query = "Wie wird das Wetter morgen?"
response = openai.Embedding.create(input=query, model="text-embedding-ada-002")
query_embedding = response['data'][0]['embedding']

# Suche im FAISS-Index
k = 2  # Anzahl der nächstgelegenen Nachbarn
D, I = index.search(np.array([query_embedding]), k)

# Ausgabe der Ergebnisse
for i in I[0]:
    print(f"Ähnliches Dokument: {documents[i]}")
----

4. **Integration und Kontextaufbau**:
Durch die Integration von GPT-3 und FAISS in LangChain können wir komplexere Anwendungen erstellen, die kontextuelles Verständnis und kontextuelle Antworten umfassen:
+
[source,python]
----
# GPT-3 verwenden, um eine kontextuelle Antwort basierend auf der Suche zu generieren
def generate_response(query):
    response = openai.Embedding.create(input=query, model="text-embedding-ada-002")
    query_embedding = response['data'][0]['embedding']
    D, I = index.search(np.array([query_embedding]), 1)
    similar_doc = documents[I[0][0]]

    prompt = f"Frage: {query}\nAntwort basierend auf ähnlichem Dokument: {similar_doc}\nGib eine fundierte Antwort:"
    response = openai.Completion.create(engine="text-davinci-003", prompt=prompt, max_tokens=100)
    return response.choices[0].text.strip()

# Beispiel-Abfrage
query = "Wie wird das Wetter morgen?"
response = generate_response(query)
print(f"Antwort: {response}")
----

Durch diese Schritte können wir eine Beispielanwendung erstellen, die die kontextuelle Suche und die Textgenerierung von GPT-3 mit der Vektorspeicherung und -abfrage des FAISS-Index kombiniert. LangChain erleichtert die Integration dieser Komponenten und ermöglicht den Aufbau leistungsfähiger KI-Anwendungen.
